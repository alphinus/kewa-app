---
phase: 04-voice-notes
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/lib/transcription.ts
  - src/app/api/audio/[id]/transcribe/route.ts
  - src/app/api/audio/route.ts
autonomous: true
user_setup:
  - service: openai
    why: "Speech-to-text transcription requires OpenAI API"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard → API Keys → Create new secret key"
    account_setup:
      - url: "https://platform.openai.com/signup"
        skip_if: "Already have OpenAI account"
    dashboard_config: []
    local_dev: []

must_haves:
  truths:
    - "KEWA's explanation audio is automatically transcribed"
    - "Transcription appears in German (Hochdeutsch)"
    - "Transcription status is tracked (pending/processing/completed/failed)"
    - "Failed transcriptions can be retried"
  artifacts:
    - path: "src/lib/transcription.ts"
      provides: "OpenAI Whisper API integration"
      contains: "transcribeAudio"
    - path: "src/app/api/audio/[id]/transcribe/route.ts"
      provides: "Transcription trigger endpoint"
      exports: ["POST"]
  key_links:
    - from: "api/audio POST"
      to: "transcription service"
      via: "auto-trigger for explanation audio"
    - from: "transcription service"
      to: "OpenAI Whisper API"
      via: "fetch to api.openai.com"
    - from: "transcription result"
      to: "task_audio table"
      via: "update transcription + status"
---

<objective>
Integrate OpenAI Whisper API for automatic transcription of KEWA's explanation audio recordings.

Purpose: Convert voice instructions to text so Imeri can read them (Hochdeutsch transcription).
Output: Transcription service library and API endpoint for triggering/checking transcription.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Audio API created in 04-01:
@src/app/api/audio/route.ts
@src/types/database.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create transcription service library</name>
  <files>src/lib/transcription.ts</files>
  <action>
Create src/lib/transcription.ts:

1. TranscriptionResult type:
   - success: boolean
   - transcription?: string
   - error?: string

2. transcribeAudio function:
   - Input: audioBuffer (Buffer or ArrayBuffer), fileName (string)
   - Output: Promise<TranscriptionResult>
   - Implementation:
     a. Check OPENAI_API_KEY env var exists
     b. Create FormData with:
        - file: Blob from buffer with filename
        - model: "whisper-1"
        - language: "de" (German/Hochdeutsch)
        - response_format: "text"
     c. POST to https://api.openai.com/v1/audio/transcriptions
     d. Handle response:
        - Success: return { success: true, transcription: text }
        - Error: return { success: false, error: message }
     e. Timeout: 30 seconds (audio max 1 min, transcription fast)

3. Error handling:
   - Missing API key: return error (don't throw)
   - Network error: return error with details
   - API error (rate limit, invalid file): return error with API message

4. Export:
   - transcribeAudio function
   - TranscriptionResult type

Note: Whisper-1 model supports WebM, MP4, MP3, WAV formats directly.
No need to convert audio format before sending.
  </action>
  <verify>TypeScript compiles: npx tsc --noEmit</verify>
  <done>Transcription service wraps OpenAI Whisper API with error handling</done>
</task>

<task type="auto">
  <name>Task 2: Create transcription trigger endpoint</name>
  <files>src/app/api/audio/[id]/transcribe/route.ts</files>
  <action>
Create src/app/api/audio/[id]/transcribe/route.ts:

POST /api/audio/{id}/transcribe:
1. Validate authentication (must be KEWA - only they should trigger transcription)
2. Parse audio ID from params
3. Fetch audio record, verify exists
4. Validate audio_type is 'explanation' (emergency audio not transcribed)
5. Check transcription_status:
   - If 'processing': return 409 "Transcription already in progress"
   - If 'completed': return 200 with existing transcription (skip re-transcription)
6. Update status to 'processing'
7. Download audio file from storage (get signed URL, fetch buffer)
8. Call transcribeAudio(buffer, fileName)
9. Update database:
   - Success: transcription=text, transcription_status='completed'
   - Failure: transcription_status='failed', transcription=error message
10. Return { audio: TaskAudioWithUrl } with updated record

Error cases:
- 401 if not authenticated
- 403 if not KEWA (only KEWA can trigger transcription)
- 404 if audio not found
- 400 if audio_type is not 'explanation'
- 409 if already processing
- 500 for transcription/storage errors

This endpoint is idempotent - calling again on completed audio returns existing result.
  </action>
  <verify>API route compiles: npx tsc --noEmit</verify>
  <done>POST triggers transcription for explanation audio, updates status</done>
</task>

<task type="auto">
  <name>Task 3: Auto-trigger transcription on upload</name>
  <files>src/app/api/audio/route.ts</files>
  <action>
Modify POST /api/audio in src/app/api/audio/route.ts:

After successful audio upload (after database insert), for explanation audio only:

1. Check if audio_type === 'explanation'
2. If yes, trigger transcription asynchronously (don't await):
   - Fetch audio from storage
   - Call transcribeAudio
   - Update database with result
   - Log any errors but don't fail the upload

Implementation pattern (fire-and-forget):
```typescript
if (audioType === 'explanation') {
  // Fire-and-forget transcription
  triggerTranscription(newAudio.id).catch((err) => {
    console.error('Auto-transcription error:', err)
  })
}
```

Create helper function triggerTranscription(audioId: string):
- Fetch audio record
- Download from storage
- Call transcribeAudio
- Update database

This ensures:
- Upload completes immediately (good UX)
- Transcription happens in background
- Failure doesn't break upload
- User can manually retry via /transcribe endpoint if auto fails
  </action>
  <verify>npm run build succeeds</verify>
  <done>Explanation audio automatically triggers transcription after upload</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] src/lib/transcription.ts exports transcribeAudio function
- [ ] POST /api/audio/[id]/transcribe endpoint exists
- [ ] Auto-transcription triggered for explanation uploads
- [ ] npm run build succeeds
- [ ] TypeScript types are correct
</verification>

<success_criteria>
- All 3 tasks completed with atomic commits
- Transcription service handles OpenAI API correctly
- Auto-transcription on upload works (fire-and-forget)
- Manual transcription endpoint works for retry
- German language specified for Whisper
</success_criteria>

<output>
After completion, create `.planning/phases/04-voice-notes/04-02-SUMMARY.md`
</output>
