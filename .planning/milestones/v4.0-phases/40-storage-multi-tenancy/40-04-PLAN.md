---
phase: 40-storage-multi-tenancy
plan: 04
type: execute
wave: 3
depends_on:
  - "40-02"
  - "40-03"
files_modified:
  - supabase/scripts/migrate_storage_paths.ts
autonomous: true
requirements:
  - STOR-03
must_haves:
  truths:
    - "All existing files in task-photos, task-audio, media, and inspections buckets are copied to org-prefixed paths"
    - "All DB records with storage_path columns are updated to point to the new org-prefixed paths"
    - "JSONB checklist_items photo_storage_paths in inspections table are updated to org-prefixed paths"
    - "Old files at non-prefixed paths are removed after successful copy"
  artifacts:
    - path: "supabase/scripts/migrate_storage_paths.ts"
      provides: "One-shot migration script for existing storage files"
      contains: "copy.*remove"
  key_links:
    - from: "supabase/scripts/migrate_storage_paths.ts"
      to: "storage buckets"
      via: "supabase.storage.from(bucket).copy() then .remove()"
      pattern: "storage.*copy|storage.*remove"
    - from: "supabase/scripts/migrate_storage_paths.ts"
      to: "DB tables with storage_path"
      via: "UPDATE table SET storage_path = new_path"
      pattern: "UPDATE.*storage_path"
---

<objective>
Create and run a migration script that moves all existing storage files to org-prefixed paths and updates all database references.

Purpose: KEWA AG is the only existing organization. All files currently live at paths without an org prefix. After Plans 40-01/02/03, new uploads use org-prefixed paths. This script migrates existing files to match, ensuring no broken links.

Output: `supabase/scripts/migrate_storage_paths.ts` — a one-shot idempotent migration script.
</objective>

<execution_context>
@C:/Users/Mario Giacchino/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Mario Giacchino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/40-storage-multi-tenancy/40-RESEARCH.md
@.planning/phases/40-storage-multi-tenancy/40-01-SUMMARY.md
@.planning/phases/40-storage-multi-tenancy/40-02-SUMMARY.md
@.planning/phases/40-storage-multi-tenancy/40-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create storage path migration script</name>
  <files>supabase/scripts/migrate_storage_paths.ts</files>
  <action>
Create `supabase/scripts/migrate_storage_paths.ts` as a standalone Node.js/TypeScript script that:

1. **Setup:** Uses `@supabase/supabase-js` with service_role key (bypasses all RLS). Read SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY from environment variables or hardcode the local dev values. Add a `--dry-run` flag that logs what would happen without executing.

2. **Lookup org ID:** Query `organizations` table for the KEWA AG org:
   ```typescript
   const { data: org } = await supabase
     .from('organizations')
     .select('id')
     .eq('slug', 'kewa-ag')
     .single()
   const ORG_ID = org.id
   ```

3. **Migrate each bucket's files:**

   For each bucket (`task-photos`, `task-audio`, `media`, `inspections`):
   a. List ALL files recursively using `supabase.storage.from(bucket).list('', { limit: 1000 })`. For nested directories, recursively list subdirectories.
   b. For each file, check if the path already starts with a UUID (org prefix). Skip if already prefixed.
      - Detection: Check if the first path segment is the ORG_ID. If so, skip.
   c. Copy to new path: `supabase.storage.from(bucket).copy(oldPath, newPath)` where `newPath = \`${ORG_ID}/${oldPath}\``
   d. Remove old file: `supabase.storage.from(bucket).remove([oldPath])`
   e. Log progress: `[bucket] Migrated: oldPath -> newPath`

4. **Update DB records with storage_path columns:**

   For each table with a `storage_path` column, update all rows where storage_path does not start with the ORG_ID:

   ```typescript
   // Tables with storage_path column:
   const TABLES_WITH_STORAGE_PATH = [
     'task_photos',       // storage_path
     'task_audio',        // storage_path
     'media',             // storage_path
     'kb_attachments',    // storage_path
     'ticket_attachments', // storage_path
     'change_order_photos', // storage_path
   ]

   for (const table of TABLES_WITH_STORAGE_PATH) {
     const { data: rows } = await supabase
       .from(table)
       .select('id, storage_path')
       .not('storage_path', 'like', `${ORG_ID}/%`)

     for (const row of rows || []) {
       const newPath = `${ORG_ID}/${row.storage_path}`
       await supabase
         .from(table)
         .update({ storage_path: newPath })
         .eq('id', row.id)
     }
     console.log(`[DB] Updated ${(rows || []).length} rows in ${table}`)
   }
   ```

5. **Update inspections.signature_storage_path:**
   ```typescript
   const { data: inspections } = await supabase
     .from('inspections')
     .select('id, signature_storage_path')
     .not('signature_storage_path', 'is', null)
     .not('signature_storage_path', 'like', `${ORG_ID}/%`)

   for (const insp of inspections || []) {
     const newPath = `${ORG_ID}/${insp.signature_storage_path}`
     await supabase
       .from('inspections')
       .update({ signature_storage_path: newPath })
       .eq('id', insp.id)
   }
   ```

6. **Update JSONB checklist_items photo_storage_paths in inspections:**
   ```typescript
   const { data: allInspections } = await supabase
     .from('inspections')
     .select('id, checklist_items')
     .not('checklist_items', 'is', null)

   for (const insp of allInspections || []) {
     const items = insp.checklist_items as any[]
     if (!items || items.length === 0) continue

     let modified = false
     for (const section of items) {
       for (const item of section.items || []) {
         const paths = item.photo_storage_paths || []
         for (let i = 0; i < paths.length; i++) {
           if (!paths[i].startsWith(`${ORG_ID}/`)) {
             paths[i] = `${ORG_ID}/${paths[i]}`
             modified = true
           }
         }
       }
     }

     if (modified) {
       await supabase
         .from('inspections')
         .update({ checklist_items: items })
         .eq('id', insp.id)
     }
   }
   ```

7. **Update inspection_defects.photo_storage_paths:**
   ```typescript
   const { data: defects } = await supabase
     .from('inspection_defects')
     .select('id, photo_storage_paths')
     .not('photo_storage_paths', 'eq', '{}')

   for (const defect of defects || []) {
     const paths = defect.photo_storage_paths || []
     let modified = false
     const newPaths = paths.map((p: string) => {
       if (!p.startsWith(`${ORG_ID}/`)) {
         modified = true
         return `${ORG_ID}/${p}`
       }
       return p
     })

     if (modified) {
       await supabase
         .from('inspection_defects')
         .update({ photo_storage_paths: newPaths })
         .eq('id', defect.id)
     }
   }
   ```

8. **Summary:** Log total files migrated per bucket, total DB rows updated per table.

**Idempotency:** Every check uses "does NOT start with ORG_ID/" — running the script twice is safe. Files already prefixed and DB rows already updated are skipped.

**Error handling:** Wrap each file copy+remove in try/catch. Log errors but continue with remaining files. At the end, report how many files failed.

Add a shebang and instructions at the top:
```typescript
#!/usr/bin/env npx tsx
/**
 * One-shot migration: prefix all storage paths with organization ID.
 *
 * Usage:
 *   SUPABASE_URL=http://127.0.0.1:54321 SUPABASE_SERVICE_ROLE_KEY=... npx tsx supabase/scripts/migrate_storage_paths.ts
 *   Add --dry-run to preview changes without executing.
 */
```
  </action>
  <verify>
1. Run with `--dry-run` flag to verify it lists files and DB rows that would be migrated.
2. Run without flag to execute migration (on local dev).
3. After running, verify: `SELECT count(*) FROM task_photos WHERE storage_path NOT LIKE '{ORG_ID}/%'` returns 0 for all tables.
4. Verify files in buckets are at new paths via `supabase.storage.from(bucket).list(ORG_ID)`.
  </verify>
  <done>All existing storage files are copied to org-prefixed paths. All DB records (storage_path columns, JSONB photo paths) updated. Old non-prefixed files removed. Script is idempotent on re-run.</done>
</task>

</tasks>

<verification>
1. Script runs without errors on local dev environment
2. All storage_path columns in all tables start with ORG_ID prefix
3. All JSONB photo paths in inspections.checklist_items and inspection_defects.photo_storage_paths start with ORG_ID prefix
4. inspections.signature_storage_path starts with ORG_ID prefix where not null
5. Storage bucket files exist at new org-prefixed paths
6. No broken image/file links in the UI after migration
</verification>

<success_criteria>
- Migration script exists and is idempotent
- All existing files accessible under org-prefixed paths
- All DB references updated to match new paths
- No broken links in the application UI
</success_criteria>

<output>
After completion, create `.planning/phases/40-storage-multi-tenancy/40-04-SUMMARY.md`
</output>
